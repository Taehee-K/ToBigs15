{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Playing Atari with Deep Reinforcement Learning Review\n",
    "\n",
    "**기본적인 강화학습 원리 및 용어:** <br>\n",
    "강화학습시 state, action 번갈아 가며 목표 달성(state, action, reward 형태)<br>\n",
    "agent: '행동' 결정<br>\n",
    "environment: 상태 전환과 보상액 결정 <br>\n",
    "누적 보상액을 최대화하는 행동을 선택, 이때 agent가 행동 선택시 사용하는 규칙을 policy라고 함\n",
    "\n",
    "* Reinforcement Learning(강화학습, RL)을 활용하여 처음으로 high-dimensional sensory output을 가지고 control policies를 학습한 첫 딥러닝 모델\n",
    "* CNN 구조에 변형된 형태의 Q-Learning으로 학습시킨 모델\n",
    "* Architecture와 learning algorithm의 변화 없이 7개의 Atari 2600게임에 적용\n",
    "    - 6번의 경우에서는 이전의 모델들보다 더 뛰어난 성능을 보여주었고\n",
    "    - 3번의 경우에서는 사람보다 더 뛰어난 성능을 보여주었다.\n",
    "\n",
    "## Introduction\n",
    "* Deep Learning에서는 이렇게 얻어진 high-level features들을 CNN, MLP, restricted Boltzmann machines, RNN등의 다양한 NN 구조에 적용시켜 supervised/unsupervised learning 을 진행 -> 결과는 성공적, Deep Learning을 RL에 적용하는 연구를 진행하게 됨\n",
    "* **But** Reinforcement Learning에 Deep Learning을 사용할 시 문제점 \n",
    "    - deep learning은 labeled 많은 데이터 필요로 함\n",
    "    - Reinforcement Learning은 frequently sparce, noisy, delayed 된 데이터의 scalar reward 값으로부터 학습 가능해야 함\n",
    "    => RL에서의 action과 reward 사이에 delay 생김\n",
    "    - deep learning에서 데이터 서로 independent\n",
    "    => RL에서 현재 취하는 행동 다음 상태의 reward에 영향 끼침, data 서로 correlated\n",
    "    - Deep Learning에서는 데이터의 분포가 fixed 되어 있다고 가정\n",
    "    => RL에서 data distribution은 알고리즘이 새로운 behavior를 학습할 때 마다 바뀐다 \n",
    "* 해당 논문에서는 CNN을 이용하여 성공적으로 Control Policy 를 학습하여 Reinforcement Learning을 할 수 있음을 보임\n",
    "\n",
    "## Background\n",
    "* Agent가 환경(E)와 상호작용하는 task 가 있을 때\n",
    "    - Agent 각 time-stamp 마다 A = {1, ..., K} 중에서 한 가지 tjsxor\n",
    "    - Action 전달시 Emulator 게임 점수 수정 (Agent 내부 상태 알지 X)\n",
    "    - Agent 현재 화면을 나타내는 이미지와 reward 전달받음 \n",
    "* Agent 현재의 화면만을 보고 학습하기에는 어려움 있음 => **action, observation sequence 고려**, MDP(Markov Decision Proccess)에 RL 적용\n",
    "* Agent는 future reward를 최대화하는 action을 선택  \n",
    "    - $R_t = \\sum_{t' = t}^T (\\gamma^{t'-t}r_t) $\n",
    "    - $Q^*(s,a) = max_\\pi E[R_t|s_t = s, a_t = a, \\pi]$   \n",
    "\n",
    "    위의 식을 bellman equation 에 근거하여 변형할 시    \n",
    "    - $Q^*(s,a) = E_{s'~ \\epsilon}[r + \\gamma max_{a'} Q^*(s',a')|s,a]$\n",
    "    \n",
    "    위 식이 수렴할 때 까지 반복하면 optimal action-value function 을 도출\n",
    "\n",
    "    Bellman Equation을 이용한 Loss function\n",
    "    - $L_i(\\theta_i) = E_{s,a~\\rho(.)}[(y_i - Q(s,a;\\theta_i))^2]$\n",
    "    Neural Network Function approximator 로 weight $theta$ 사용     \n",
    "    Q-net 반복시 Loss Function 최소화하는 방향으로 학습\n",
    "\n",
    "    위 과정을 통해 얻어지는 gradient\n",
    "    - $\\nabla_\\theta L_i(\\theta_i) = E_{s,a~\\rho(.);s'~\\epsilon}[(r + \\gamma max_{a'} Q^*(s',a';\\theta_{i-1}) - Q(s,a;\\theta_i)\\nabla_\\theta Q(s,a;\\theta_i)]$          \n",
    "    stochastic gradient descent 활용하여 손실함수 최적화 -> 계산의 편리  \n",
    "* 위의 알고리즘은 model - free 알고리즘이다 (Emulation 고려X)\n",
    "* Off Policy: Learning Policy와 Behavior Policy가 다름\n",
    "    \n",
    "## Deep Reinforcement Learning\n",
    "* 목표: Reinforced Learning을 SGD를 사용해 효율적으로 training data를 처리 후 RGB 이미지를 input으로 하는 DNN에 connect 하는 것\n",
    "* 각 time-step에서 agent의 경험을 저장한 experience replay 이용     \n",
    "![alt-text](DQN.png)\n",
    "* 장점: \n",
    "    - step의 Experience 가 많은 weight update에 재사용됨, 기존에 weight update에 experience를 한 번만 사용하는 방법보다 data efficient\n",
    "    - e-greedy 알고리즘을 통해 randomize -> 데이터 correlation을 깨뜨려 update 효율 향상\n",
    "    - (off policy 사용) experience relay를 사용함으로서 behavior distribution 이 균형 이룸 -> parameter 학습시 좋음\n",
    "* 단점: 메모리 사이즈가 제한되어 있어 계속 최근의 tranition을 덮어씌워야 한다\n",
    "\n",
    "### Preprocessing\n",
    "* 계산량 줄이기 위해 bisc processing step 적용\n",
    "* gray-scale, 110 x 84 pixel image 로 downsampling\n",
    "* 84 * 84 로 잘라내어 final output 추출\n",
    "\n",
    "## Experiments\n",
    "* reward: \n",
    "    - positive: 1\n",
    "    - negative: -1\n",
    "    - no change: 0\n",
    "* RMSProp algorithm 사용\n",
    "* frame-skipping technique 사용해 연산량 줄임\n",
    "\n",
    "## Conclusion\n",
    "* RL에 raw pixel을 input 으로 한 Deep Learning을 적용한 모델을 소개함\n",
    "* Stochastic mini batch update에 Experience Replay Memory를 적용해 변형된 Q-learning 소개\n",
    "\n",
    "\n",
    "참고 자료 \n",
    "https://dongminlee.tistory.com/3\n",
    "http://sanghyukchun.github.io/90/\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}